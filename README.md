# wine-quality-classification

This repository contains the machine learning project notebook focusing on wine quality classification. The objective of this project is to develop and evaluate several classification models (KNN, Naive Bayes, Decision Tree, and SVM) to predict whether a wine is considered "Good Quality" or "Bad Quality" based on its physicochemical properties.
The methodology includes detailed data preprocessing, iterative feature selection based on importance, extensive hyperparameter tuning, and comprehensive model evaluation.

ðŸ’¾ Dataset
The project uses the winequality.csv dataset.

Data Attributes (Features)
The input features describe the chemical properties of the wine, including:
â€¢ Fixed acidity
â€¢ Volatile acidity
â€¢ Citric acid
â€¢ Residual sugar
â€¢ Chlorides
â€¢ Free sulfur dioxide
â€¢ Total sulfur dioxide
â€¢ Density
â€¢ pH
â€¢ Sulphates
â€¢ Alcohol

Target Variable
The original 'quality' score was transformed to create a binary classification label, 'quality_binary':
â€¢ Good Quality (1): Original quality score â‰¥7
â€¢ Bad Quality (0): Original quality score <7

âœ¨ Methodology and Pipeline
The analysis pipeline followed rigorous steps including data cleaning, feature engineering, and iterative model testing:
1. Data Cleaning and Preprocessing
â€¢ Null Value Check: The dataset was confirmed to have no null values.
â€¢ Duplicate Handling: Duplicate rows were identified and removed from the dataset.
â€¢ Data Splitting: The dataset was split into training (70%) and testing (30%) sets for model development and final evaluation.
â€¢ Feature Scaling: StandardScaler was employed for certain models (specifically SVM) to normalize the features before training.
2. Feature Selection
Feature importance was assessed using an ExtraTreesClassifier model trained on the data. This technique helped determine the optimal subset of features for improving model performance.
â€¢ The models were repeatedly evaluated across feature subsets ranging from 2 up to 9 features to observe the impact on performance metrics. 

. Model Implementation and Tuning
The following classification algorithms were implemented and rigorously tested:
a) K-Nearest Neighbors(KNN): Tested across various feature subsets (2F to 9F).
Optimal k value was determined (e.g., k=17 based on cross-validation accuracy plot).
b) Naive Bayes (GaussianNB): Trained and evaluated early in the process. Standard training and metrics calculation.
c) Decision Tree: Trained using the 'gini' criterion. Tested across various feature subsets (2F to 9F).
d) Support Vector Machines (SVM): Extensive evaluation was performed comparing different kernel types and hyperparameters. Linear SVM: Tested various C values (0.01, 0.1, 1.0). Polynomial SVM: Tested degrees (1 through 5). RBF Kernel SVM: Grid search conducted for Gamma and C values.

The Radial Basis Function (RBF) Kernel SVM showed the highest accuracy among the tested SVM configurations and was selected as the final model for further iterative feature testing.

ðŸ“Š Evaluation and Results
All models were evaluated using the following classification metrics:
â€¢ Accuracy Score
â€¢ Precision
â€¢ Recall
â€¢ F1 Score
â€¢ Confusion Matrix
The evaluation consistently tracked how performance metrics changed as the number of input features increased.
