# wine-quality-classification

üìù Project Overview
This repository contains the machine learning implementation for a binary classification task focused on predicting the quality of wine based on its physicochemical properties.<br> The main goal is to categorize wine samples as either "Good Quality" or "Bad Quality". The methodology includes extensive preprocessing, feature importance assessment, and comparison across multiple algorithms, including iterative testing of various feature subsets.<br>

--------------------------------------------------------------------------------
üìä Dataset and Target Variable
The project uses the winequality.csv dataset.<br>
Features (Physicochemical Properties)
The model inputs rely on eleven features:
‚Ä¢ Fixed acidity<br>
‚Ä¢ Volatile acidity<br>
‚Ä¢ Citric acid<br>
‚Ä¢ Residual sugar<br>
‚Ä¢ Chlorides<br>
‚Ä¢ Free sulfur dioxide<br>
‚Ä¢ Total sulfur dioxide<br>
‚Ä¢ Density<br>
‚Ä¢ pH<br>
‚Ä¢ Sulphates<br>
‚Ä¢ Alcohol<br>
Target Transformation
The original quantitative 'quality' score was converted into a binary target variable, 'quality_binary':<br>
‚Ä¢ Good Quality (1): Original quality score ‚â•7<br>
‚Ä¢ Bad Quality (0): Original quality score <7<br>

--------------------------------------------------------------------------------
üõ†Ô∏è Data Pipeline and Methodology
1. Data Preprocessing
‚Ä¢ Null Check: The dataset was verified to contain zero null values.<br>
‚Ä¢ Duplicate Handling: Duplicate rows present in the initial dataset were removed.<br>
‚Ä¢ Data Split: The data was partitioned into a training set (70%) and a testing set (30%).<br>
‚Ä¢ Feature Scaling: Standard scaling using StandardScaler was applied to the features prior to training distance-based models like KNN and SVM.<br>
2. Feature Selection
Feature importance was analyzed using an ExtraTreesClassifier model, which ranked the influence of the 11 attributes on predicting quality. The importance visualization guided iterative model testing using various subsets of features (e.g., 2, 3, 4, up to 9 features) to find the combination yielding the highest performance metrics.<br>
3. Model Implementation and Tuning
Three primary model types were deployed and evaluated:<br>
K-Nearest Neighbors (KNN)
‚Ä¢ The optimal number of neighbors (k) was searched for (e.g., k=17 was determined as optimal based on cross-validation accuracy plot).
‚Ä¢ Trained and evaluated iteratively across various feature subsets.<br>
Decision Tree
‚Ä¢ The classifier was instantiated using the 'gini' criterion.
‚Ä¢ Tested across varying feature subsets.<br>
Support Vector Machines (SVM)
The SVM section involved the most comprehensive evaluation, testing three kernel types and performing hyperparameter tuning:
1. Linear Kernel: Tested different C values (0.01, 0.1, 1.0).
2. Polynomial Kernel: Evaluated different polynomial degrees (1 through 5).
3. Radial Basis Function (RBF) Kernel: Optimized using a search over multiple gamma and C values.<br>
The RBF kernel generally achieved the highest accuracy compared to the Linear and Polynomial kernels and was therefore selected as the best performing SVM approach.<br>

--------------------------------------------------------------------------------
üìà Evaluation Metrics
All models were measured using a standard suite of classification metrics:<br>
‚Ä¢ Accuracy Score<br>
‚Ä¢ Precision<br>
‚Ä¢ Recall<br>
‚Ä¢ F1 Score<br>
‚Ä¢ Confusion Matrix<br>
